# -*- coding: utf-8 -*-
"""tfx_pipelines.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PH9ichKMU1nm4PEsmBVqAuP-1tjZ-fBh

# **Submission 2 Proyek Pengembangan dan Pengoperasian Sistem Machine Learning**

## **Data Preparation (Kaggle)**

### **Dataset Summary**

This dataset corresponds to an augmented version of the [**Electrical Grid Stability Simulated Dataset**](https://archive.ics.uci.edu/ml/datasets/Electrical+Grid+Stability+Simulated+Data+#), created by Vadim Arzamasov (Karlsruher Institut f√ºr Technologie, Karlsruhe, Germany) and donated to the [University of California (UCI) Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)

### **Kaggle Environment Variables**
"""

"""## **Library**"""

# import library

"""## **Set Pipeline Variable**"""

# pipeline name
import os
import sys
import pandas as pd
from typing import Text
from absl import logging
from tfx.orchestration import metadata, pipeline
from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner
PIPELINE_NAME = "smart-grid-pipeline"


# pipeline inputs
DATA_ROOT = "data"
TRANSFORM_MODULE_FILE = "modules/smart_grid_transform.py"
TUNER_MODULE_FILE = "modules/smart_grid_tuner.py"
TRAINER_MODULE_FILE = "modules/smart_grid_trainer.py"

# pipeline outputs
OUTPUT_BASE = "output"

serving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')
pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)
metadata_path = os.path.join(pipeline_root, "metadata.sqlite")

"""## **Data Loading**"""

# read csv
pd.reset_option('^display.', silent=True)

DATA_PATH = 'data'
df = pd.read_csv(os.path.join(DATA_PATH, 'smart_grid_stability_augmented.csv'))
df.head()

"""The dataset chosen for this machine learning exercise has a synthetic nature and contains results from simulations of grid stability for a reference 4-node star network, as described in 1.2.

The original dataset contains 10,000 observations. As the reference grid is symetric, the dataset can be augmented in 3! (3 factorial) times, or 6 times, representing a permutation of the three consumers occupying three consumer nodes. The augmented version has then **60,000 observations**. It also contains **12 primary predictive features** and two dependent variables.

**Predictive features:**
<ol>
    <li>'tau1' to 'tau4': the reaction time of each network participant, a real value within the range 0.5 to 10 ('tau1' corresponds to the supplier node, 'tau2' to 'tau4' to the consumer nodes);</li>
    <li>'p1' to 'p4': nominal power produced (positive) or consumed (negative) by each network participant, a real value within the range -2.0 to -0.5 for consumers ('p2' to 'p4'). As the total power consumed equals the total power generated, p1 (supplier node) = - (p2 + p3 + p4);</li>
    <li>'g1' to 'g4': price elasticity coefficient for each network participant, a real value within the range 0.05 to 1.00 ('g1' corresponds to the supplier node, 'g2' to 'g4' to the consumer nodes; 'g' stands for 'gamma');</li>
</ol>

**Dependent variables:**
<ol>
    <li>'stab': the maximum real part of the characteristic differentia equation root (if positive, the system is linearly unstable; if negative, linearly stable);</li>
    <li>'stabf': a categorical (binary) label ('stable' or 'unstable').</li>
</ol>

As there is a direct relationship between 'stab' and 'stabf' ('stabf' = 'stable' if 'stab' <= 0, 'unstable' otherwise), 'stab' will be dropped and 'stabf' will remain as the sole dependent variable.
"""

# info
df.info()

# label count
df['stabf'].value_counts(normalize=True)

# drop columns
df.drop('stab', axis=1, inplace=True)

# encode
map_label = {'unstable': 0, 'stable': 1}
df['stabf'] = df['stabf'].replace(map_label)
df.head()

# save data
DATA_PATH = 'data'

# create new directory to save new csv
if not os.path.exists(DATA_PATH):
    os.makedirs(DATA_PATH)

# export to csv
df.to_csv(os.path.join(DATA_PATH, 'smart_grid_stability.csv'), index=False)

# remove old dataframe
os.remove('./data/smart_grid_stability_augmented.csv')

# let's see dataframe
df = pd.read_csv(os.path.join(DATA_PATH, 'smart_grid_stability.csv'))
df.head()

"""## **ML Pipelines**"""


def init_local_pipeline(
    components, pipeline_root: Text
) -> pipeline.Pipeline:

    logging.info(f"Pipeline root set to: {pipeline_root}")
    beam_args = [
        "--direct_running_mode=multi_processing"
        # 0 auto-detect based on on the number of CPUs available
        # during execution time.
        "----direct_num_workers=0"
    ]

    return pipeline.Pipeline(
        pipeline_name=PIPELINE_NAME,
        pipeline_root=pipeline_root,
        components=components,
        enable_cache=True,
        metadata_connection_config=metadata.sqlite_metadata_connection_config(
            metadata_path
        ),
        eam_pipeline_args=beam_args
    )


if __name__ == "__main__":
    logging.set_verbosity(logging.INFO)

    from modules.components import init_components

    components = init_components(
        DATA_ROOT,
        transform_module=TRANSFORM_MODULE_FILE,
        tuner_module=TUNER_MODULE_FILE,
        training_module=TRAINER_MODULE_FILE,
        training_steps=5000,
        eval_steps=1000,
        serving_model_dir=serving_model_dir,
    )

    pipeline = init_local_pipeline(components, pipeline_root)
    BeamDagRunner().run(pipeline=pipeline)
